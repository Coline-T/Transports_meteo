# Projet Int√©gr√© : Etat du trafic rennais en fonction de la m√©t√©o

## Technologies Utilis√©es

### Langage

![Python](https://img.shields.io/badge/Python-3.10.12-blue?logo=python&logoColor=white)

### Frameworks et Outils de D√©veloppement

![Docker](https://img.shields.io/badge/Docker-20.10.7-blue?logo=docker&logoColor=white)

### Cloud & Bases de Donn√©es

![Elasticsearch](https://img.shields.io/badge/Elasticsearch-8.15.0-blue?logo=elasticsearch&logoColor=white)
![Kibana](https://img.shields.io/badge/Kibana-8.15.0-orange?logo=kibana&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-5.0-green?logo=mongodb&logoColor=white)

### Biblioth√®ques de Donn√©es & Machine Learning

![Pandas](https://img.shields.io/badge/Pandas-1.5.2-brightgreen?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-1.21.0-blue?logo=numpy&logoColor=white)

### Outils de Visualisation

![Kibana](https://img.shields.io/badge/Kibana-8.15.0-orange?logo=kibana&logoColor=white)

### Conteneurisation et D√©ploiement

![Docker](https://img.shields.io/badge/Docker-20.10.7-blue?logo=docker&logoColor=white)

### Outils de D√©bogage et de Terminal

![IPython](https://img.shields.io/badge/IPython-8.0.0-blue?logo=ipython&logoColor=white)

---
Ces outils ont √©t√© utilis√©s pour le d√©veloppement du projet sur l'√©tat du trafic rennais, visant √† ing√©rer, transformer, et analyser les donn√©es du trafic en temps r√©el pour obtenir des informations sur les habitudes des usagers et rep√©rer les heures d'affluence ainsi qu'analyser pour savoir si le trafic est influenc√© par la m√©t√©o ou non. Le traitement des donn√©es en temps r√©el est facilit√© par Airflow, ensuite nos donn√©es ont √©t√© int√©gr√©s dans un DataLake sous MongoDB.

## Objectif du Projet
Ce projet vise √† analyser l‚Äôimpact des conditions m√©t√©orologiques et des niveaux de pollution sur le trafic routier afin de proposer des solutions pour am√©liorer la gestion de la mobilit√©, r√©duire les congestions et limiter les risques d‚Äôaccidents. Ce projet nous am√®ne donc √† nous demander : 
En quoi les conditions m√©t√©orologiques et les niveaux de pollution influencent-ils le trafic routier ?


## üé≠ Mes cibles

Mes cibles principales incluent :

- **R√©seau de route rennais** qui souhaitent surveiller le trafic sur leurs route et rep√©rer si il y a des jours o√π des routes sont plus emprunt√©s que d'autres.

- **M√©tropole rennaise** qui souhaitent suivre si les routes sont plus ou moins emprunt√©s certains jours ou non.

- **Analystes de Donn√©es et Chercheurs** qui souhaitent √©tudier les tendances de fr√©quentations des clients en fonction de circonstances ext√©rieurs (exemple : m√©t√©o et pollution de l'air)


## Architecture du Projet 

```
.
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îî‚îÄ‚îÄ ??
‚îú‚îÄ‚îÄ data-ingestion-kedro ??
‚îÇ   ‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ session_store.db
‚îÇ   ‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ tests
‚îú‚îÄ‚îÄ Airflow
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt+
‚îÇ   ‚îú‚îÄ‚îÄ dags
‚îÇ       ‚îú‚îÄ‚îÄ dag.py
‚îÇ   ‚îú‚îÄ‚îÄ script
‚îÇ       ‚îú‚îÄ‚îÄ entrypoint.sh
‚îú‚îÄ‚îÄ data_collection
‚îÇ   ‚îú‚îÄ‚îÄ getAPI.py
‚îú‚îÄ‚îÄ docs ??
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ ELK
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ import_to_elasticsearch.py
‚îú‚îÄ‚îÄ ENV
‚îÇ   ‚îú‚îÄ‚îÄ bin
‚îÇ   ‚îú‚îÄ‚îÄ etc
‚îÇ   ‚îú‚îÄ‚îÄ include
‚îÇ   ‚îú‚îÄ‚îÄ lib
‚îÇ   ‚îú‚îÄ‚îÄ lib64 -> lib
‚îÇ   ‚îú‚îÄ‚îÄ Scripts
‚îÇ       ‚îú‚îÄ‚îÄ creation_csv.py
‚îÇ   ‚îú‚îÄ‚îÄ pyvenv.cfg
‚îÇ   ‚îî‚îÄ‚îÄ share
## Finir en fonction de ce qu'on rajoute
‚îú‚îÄ‚îÄ image-1.png
‚îú‚îÄ‚îÄ image-2.png
‚îú‚îÄ‚îÄ image-3.png
‚îú‚îÄ‚îÄ image-4.png
‚îú‚îÄ‚îÄ image.png
‚îú‚îÄ‚îÄ kafka
‚îú‚îÄ‚îÄ kedro-airflow
‚îÇ   ‚îú‚îÄ‚îÄ dags
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ script
‚îú‚îÄ‚îÄ notebook
‚îÇ   ‚îî‚îÄ‚îÄ EDA.ipynb
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ script
‚îÇ   ‚îú‚îÄ‚îÄ getApi_Alim.py
‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py
‚îú‚îÄ‚îÄ sentiment_analysis_kafka
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ sentiment_analysis.py
‚îî‚îÄ‚îÄ spark
    ‚îú‚îÄ‚îÄ kafka_to_spark.py
    ‚îî‚îÄ‚îÄ script
```


![alt text](image.png)

### Workflow et Sch√©ma d'Architecture

1. **Ingestion des Donn√©es de l'Etat du trafic rennais** :
   - Extraction des donn√©es sur l'Etat du trafic Rennais en temps r√©el via l'API "Etat du trafic en temps r√©el" disponible sur le site data rennes m√©tropole puis envoi des donn√©es dans Mongo DB.

2. **Ingestion des donn√©es M√©t√©o** :
   - Extraction des donn√©es m√©t√©o via l'API disponible sur Open Weather Data puis envoi des donn√©es dans Mongo DB.

3. **Ingestion des donn√©es de la pollution de l'air** :
   - Extraction des donn√©es de la pollution de l'air via l'API disponible sur Open Weather Data puis envoi des donn√©es dans Mongo DB.

4. **Traitement des Donn√©es** :
   - **Transformation des Donn√©es** : Dans un programme python, on va chercher nos donn√©es pr√©sentes dans nos collections mongoDB et on les ressort sous la forme d'un fichier CSV. Dans ce m√™me programme, on traite nos donn√©es pour avoir toutes les variables n√©cessaires.

5. **Indexation et Stockage** :
   - Les donn√©es enrichies sont stock√©es dans Elasticsearch, index√©es par ...

6. **Visualisation et Analyse** :
   - Kibana est utilis√© pour cr√©er des tableaux de bord interactifs, permettant de suivre l'usage des transports en commun en fonction de la m√©t√©o et de la pollution de l'air.

## Fonctionnalit√©s du Projet

1. **Suivi de l'√©tat du trafic Rennais**
   - **Objectif** : Suivre l'√©tat du trafic rennais avant de conna√Ætre les jours et heures d'affluence.
   - **Description** : Il est important de suivre l'√©tat du trafic afin de pouvoir l'am√©liorer en proposant des d√©viations aux usagers en cas de forte affluence, ce qui permet de limiter le risque d'accidents et de sur-accidents qui ont entra√Æn√©s les jours de forte affluence.

2. **Corr√©lation entre l'√©tat du trafic Rennais et la m√©t√©o sur une m√™me p√©riode**
   **Objectif** : Analyser l‚Äôimpact des conditions m√©t√©orologiques et des niveaux de pollution sur le trafic routier afin de proposer des solutions pour am√©liorer la gestion de la mobilit√©, r√©duire les congestions et limiter les risques d‚Äôaccidents.
   - **Description** : Les conditions m√©t√©orologiques et la pollution influencent directement le trafic routier, impactant la s√©curit√©, la fluidit√© et les comportements des usagers. Analyser ces interactions permettrait d‚Äôoptimiser la gestion de la mobilit√© urbaine.


## D√©roulement Technique du Projet

### **√âtapes d'installation :**

1. **Cloner le d√©p√¥t :**
   ```bash
   git clone https://github.com/Coline-T/Transports_meteo
   cd Transports_meteo
   ```

2. **Cr√©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les d√©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

**Configurer les variables d'environnement :**
   Cr√©ez un fichier `.env` et renseignez les informations de connexion MongoDB , OPENAI , le topic kafka , le lien de l'api et Elasticsearch :
   ```env
MONGO_USERNAME="******"
MONGO_PASSWORD="******"
MONGO_DBNAME="*******"
MONGO_URI="*********"
API_URL=https://data.rennesmetropole.fr/api/explore/v2.1/catalog/datasets/etat-du-trafic-en-temps-reel/records
OPENAI_API_KEY="*******"
KAFKA_BROKER=localhost:9092"******"
KAFKA_TOPIC="*******"
   ```

### Sous-Projet : Ingestion et Pr√©paration des Donn√©es

Cette partie du projet est un sous-projet sp√©cifique √† l'ingestion et √† la pr√©paration des donn√©es, inclus dans notre projet global Transports_Meteo. Deux pipelines Kedro ont √©t√© mis en place pour g√©rer ces donn√©es et les rendre disponibles pour l'analyse et la visualisation :

#### Pipeline ETL

Ce pipeline collecte les donn√©es brutes √† partir de l'API, les transforme via des √©tapes de nettoyage et d'enrichissement, puis les stocke dans une base de donn√©es MongoDB. Le stockage dans MongoDB permet de centraliser les donn√©es transform√©es pour une utilisation ult√©rieure, facilitant ainsi les op√©rations d'analyse et de visualisation.

![alt text](image-4.png)

### **Ex√©cuter localement :**
- **Ex√©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

### Extraction et Ingestion
   - **Donn√©es des diff√©rents API** : Extraction des donn√©es sanitaires avec Python et envoi dans MongoDB.

### Stockage et Indexation avec Elasticsearch
   - Stockage des donn√©es des transports rennais, des donn√©es m√©t√©o et des donn√©es de la pollution de l'air dans Elasticsearch.

### Visualisation avec Kibana
   - Cr√©ation de tableaux de bord pour :
     - ...

## Analyses et Indicateurs Attendus ---> A FAIRE

1. **...** : ...
2. **...** : ...
3. **...** : ...
4. **...** : ...

## Exemples de Cas d'Usage

- **Pour les autorit√©s** : Prioriser les contr√¥les dans les zones ou √©tablissements avec des niveaux d'hygi√®ne et de satisfaction faible.
- **Pour les restaurateurs** : Identifier les aspects (hygi√®ne ou service) √† am√©liorer pour r√©pondre aux attentes des clients.
- **Pour les analystes** : Suivre les tendances r√©gionales en mati√®re de conformit√© sanitaire et de satisfaction client.

## D√©ploiement

- **Docker** : Conteneurisation des services (Kafka, Spark, Elasticsearch, Kibana) pour simplifier le d√©ploiement et le scaling.
- **Configurations** : Variables d‚ÄôAPI et param√®tres de stockage configurables via des fichiers `.env`.
- **Automatisation** : Script de d√©ploiement pour ex√©cuter le pipeline complet.


## Visualisation des Donn√©es avec Kibana

Les donn√©es collect√©es et import√©es dans Elasticsearch  sont visualis√©es dans Kibana pour une analyse approfondie. Voici un aper√ßu de certaines visualisations cr√©√©es pour explorer les avis clients et leurs sentiments.

![alt text](image-6.png)

![alt text](image-5.png)



##  üìú Conclusion <a name="conclusion"></a>

L'application Realtime Restaurant Insights s'est av√©r√©e √™tre un atout consid√©rable pour les acteurs de la restauration cherchant √† comprendre et √† exploiter les retours clients en temps r√©el. Gr√¢ce √† l'int√©gration harmonieuse d'outils tels que Kafka pour l‚Äôingestion de donn√©es en temps r√©el, Apache Spark pour le traitement, et Elasticsearch et Kibana pour l‚Äôindexation et la visualisation, l'application permet une exploitation rapide et efficace des donn√©es critiques.

Cette solution offre aux restaurateurs une capacit√© in√©dite de suivre la satisfaction client, d‚Äôidentifier les probl√©matiques de mani√®re proactive, et de mettre en ≈ìuvre des actions correctives imm√©diates. Les gestionnaires de cha√Ænes peuvent obtenir une vue d‚Äôensemble de leurs multiples √©tablissements, facilitant une gestion centralis√©e tout en gardant un ≈ìil sur chaque restaurant. Cette vision consolid√©e am√©liore non seulement la qualit√© du service, mais permet aussi une prise de d√©cision fond√©e sur des informations v√©rifi√©es et actuelles.

En utilisant l‚ÄôAPI d‚ÄôOpenAI pour analyser les sentiments des avis clients, l'application est capable de transformer de simples commentaires en indicateurs concrets, fournissant des insights sur les aspects positifs et n√©gatifs du service et des produits. Cela aide non seulement √† rehausser l'exp√©rience client, mais permet √©galement aux √©quipes marketing d‚Äôorienter leurs strat√©gies de mani√®re plus personnalis√©e et pertinente.

Les fonctionnalit√©s de visualisation des donn√©es, avec Kibana, apportent une dimension interactive qui permet de transformer des volumes importants de donn√©es en tableaux de bord intuitifs. Ces visualisations permettent aux utilisateurs d'explorer les tendances, de suivre la satisfaction des clients en temps r√©el, et de prendre des d√©cisions √©clair√©es.

En somme, l‚Äôapplication "Realtime Restaurant Insights" se positionne comme un outil essentiel pour quiconque souhaite rester comp√©titif dans le secteur de la restauration. Elle aide √† optimiser la satisfaction client, am√©liorer la qualit√© des services, et exploiter les retours clients de mani√®re constructive. En mettant la donn√©e au centre de la prise de d√©cision, cette solution repr√©sente une avanc√©e majeure vers une gestion proactive et ax√©e sur les r√©sultats pour le secteur de la restauration.



üöß Difficult√©s Rencontr√©es

- **Quota Limit√© pour l'API d'OpenAI** 
Une des principales difficult√©s rencontr√©es concernait l'utilisation de l'API d'OpenAI pour l'analyse des sentiments. L'acc√®s √† l'API est limit√© par un quota d'utilisation, ce qui a parfois restreint le traitement de grands volumes de donn√©es en temps r√©el. Ce quota a n√©cessit√© des ajustements dans la fr√©quence des appels API et une priorisation des avis clients √† analyser, surtout en p√©riode de forte activit√©. En cons√©quence, une strat√©gie de gestion de quota a d√ª √™tre mise en place, impliquant notamment la mise en cache des r√©sultats et l'utilisation s√©lective de l'API pour les avis les plus pertinents.

![alt text](image-1.png)

## Am√©liorations Futures

1. **Machine Learning pour la pr√©diction des niveaux de conformit√©** : Utilisation de mod√®les pour anticiper les besoins d'inspection.
2. **Int√©gration d'autres sources d'avis (r√©seaux sociaux)** : Agr√©gation d'avis de sources vari√©es pour enrichir les donn√©es.
3. **D√©veloppement d‚Äôune API** : Fournir un acc√®s en temps r√©el aux indicateurs de qualit√© des √©tablissements pour des applications externes.

---

##  üìä Docs <a name="documentation"></a>
j'ai document√© plusieurs √©tapes critiques du projet :

**Airflow**  est utilis√© pour orchestrer les pipelines de collecte de donn√©es via des DAGs. Un exemple de DAG est utilis√© pour envoyer nos donn√©es de MongoDB vers Kafka. Ce script Airflow s'ex√©cute toutes les 8 heures. Voici une images du  DAG :

![alt text](image-3.png)

## Contributeurs

- Solenn COULON (@solennCoulon17): Data engineer -**solenn.coulon@supdevinci-edu.fr**
- Coline TREILLE (@Coline-T) : Data analyst -**coline.treille@supdevinci-edu.fr**


## Licence

Ce projet est sous licence MIT. N'h√©sitez pas √† utiliser et modifier le code pour vos propres projets.