# Projet Int√©gr√© : Etat du trafic rennais en fonction de la m√©t√©o

## Technologies Utilis√©es

### Langage

![Python](https://img.shields.io/badge/Python-3.10.12-blue?logo=python&logoColor=white)

### Frameworks et Outils de D√©veloppement

![Docker](https://img.shields.io/badge/Docker-20.10.7-blue?logo=docker&logoColor=white)

### Cloud & Bases de Donn√©es

![Elasticsearch](https://img.shields.io/badge/Elasticsearch-8.15.0-blue?logo=elasticsearch&logoColor=white)
![Kibana](https://img.shields.io/badge/Kibana-8.15.0-orange?logo=kibana&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-5.0-green?logo=mongodb&logoColor=white)

### Biblioth√®ques de Donn√©es & Machine Learning

![Pandas](https://img.shields.io/badge/Pandas-1.5.2-brightgreen?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-1.21.0-blue?logo=numpy&logoColor=white)

### Outils de Visualisation

![Kibana](https://img.shields.io/badge/Kibana-8.15.0-orange?logo=kibana&logoColor=white)

### Conteneurisation et D√©ploiement

![Docker](https://img.shields.io/badge/Docker-20.10.7-blue?logo=docker&logoColor=white)

### Outils de D√©bogage et de Terminal

![IPython](https://img.shields.io/badge/IPython-8.0.0-blue?logo=ipython&logoColor=white)

---
Ces outils ont √©t√© utilis√©s pour le d√©veloppement du projet sur l'√©tat du trafic rennais, visant √† ing√©rer, transformer, et analyser les donn√©es du trafic en temps r√©el pour obtenir des informations sur les habitudes des usagers et rep√©rer les heures d'affluence ainsi qu'analyser pour savoir si le trafic est influenc√© par la m√©t√©o ou non. Le traitement des donn√©es en temps r√©el est facilit√© par des dags Airflow, ensuite nos donn√©es ont √©t√© int√©gr√©s dans un DataLake sous MongoDB. Ces donn√©es ont par la suite √©t√© trait√©s sous python, avant d'√™tre import√©s dans Kibana pour les utiliser dans des visualisations. 

## Objectif du Projet
Ce projet vise √† analyser l‚Äôimpact des conditions m√©t√©orologiques et des niveaux de pollution sur le trafic routier afin de proposer des solutions pour am√©liorer la gestion de la mobilit√©, r√©duire les congestions et limiter les risques d‚Äôaccidents. Ce projet nous am√®ne donc √† nous demander : 
En quoi les conditions m√©t√©orologiques et les niveaux de pollution influencent-ils le trafic routier ?


## üé≠ Mes cibles

Mes cibles principales incluent :

- **M√©tropole rennaise** qui souhaitent suivre si les routes sont plus ou moins emprunt√©s certains jours ou non afin de pouvoir am√©liorer la mobilit√© urbaine. De plus, comme les donn√©es concernant le trafic rennais sont mis en lien avec la m√©t√©o, la m√©tropole pourra √©galement conna√Ætre les impacts de la m√©t√©o sur leurs routes et le trafic et prendre des mesures afin de limiter le nombre d'accidents.

- **Analystes de Donn√©es et Chercheurs** qui souhaitent √©tudier les cons√©quences de circonstances ext√©rieurs tel que la m√©t√©o sur le trafic rennais.

## Architecture du Projet 

```
.
‚îú‚îÄ‚îÄ Airflow
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt+
‚îÇ   ‚îú‚îÄ‚îÄ dags
‚îÇ       ‚îú‚îÄ‚îÄ dag.py
‚îÇ   ‚îú‚îÄ‚îÄ script
‚îÇ       ‚îú‚îÄ‚îÄ entrypoint.sh
‚îú‚îÄ‚îÄ data_collection
‚îÇ   ‚îú‚îÄ‚îÄ getAPI.py
‚îú‚îÄ‚îÄ ELK
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ import_to_elasticsearch.py
‚îú‚îÄ‚îÄ ENV
‚îÇ   ‚îú‚îÄ‚îÄ bin
‚îÇ   ‚îú‚îÄ‚îÄ etc
‚îÇ   ‚îú‚îÄ‚îÄ include
‚îÇ   ‚îú‚îÄ‚îÄ lib
‚îÇ   ‚îú‚îÄ‚îÄ lib64 -> lib
‚îÇ   ‚îú‚îÄ‚îÄ Scripts
‚îÇ       ‚îú‚îÄ‚îÄ activate.bat
‚îÇ       ‚îú‚îÄ‚îÄ API_meteo.py
‚îÇ       ‚îú‚îÄ‚îÄ API_pollution.py
‚îÇ       ‚îú‚îÄ‚îÄ connexion_mongodb.py
‚îÇ       ‚îú‚îÄ‚îÄ creation_traitement_csv.py
‚îÇ   ‚îú‚îÄ‚îÄ pyvenv.cfg
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ dag.png
‚îú‚îÄ‚îÄ elasticsearch.png
‚îú‚îÄ‚îÄ kibana.png
‚îú‚îÄ‚îÄ schema.png
‚îú‚îÄ‚îÄ README.md
```
![alt text](image.png)

### Workflow et Sch√©ma d'Architecture

1. **Ingestion des Donn√©es de l'Etat du trafic rennais** :
   - Extraction des donn√©es sur l'Etat du trafic Rennais en temps r√©el via l'API "Etat du trafic en temps r√©el" disponible sur le site data rennes m√©tropole puis envoi des donn√©es dans Mongo DB.

2. **Ingestion des donn√©es M√©t√©o** :
   - Extraction des donn√©es m√©t√©o via l'API disponible sur Open Weather Data puis envoi des donn√©es dans Mongo DB.

3. **Traitement des Donn√©es** :
   - **Transformation des Donn√©es** : Dans un programme python, on va chercher nos donn√©es pr√©sentes dans nos collections mongoDB et on les ressort sous la forme d'un fichier CSV. Dans ce m√™me programme, on traite nos donn√©es pour avoir des variables n√©cessaires √† nos visualisations et des donn√©es propres. Par la suite, on a fusionn√© nos deux fichiers d'API en un seul fichier pour faciliter les visualisations.

4. **Indexation et Stockage** :
   - Notre fichier de donn√©es est ensuite stock√©es dans ElasticSearch, index√©es par date.

5. **Visualisation et Analyse** :
   - Kibana est utilis√© pour cr√©er des tableaux de bord interactifs, permettant de suivre l'√©tat du trafic rennais en fonction de la m√©t√©o.

## Fonctionnalit√©s du Projet

1. **Suivi de l'√©tat du trafic Rennais**
   - **Objectif** : Suivre l'√©tat du trafic rennais avant de conna√Ætre les jours et heures d'affluence.
   - **Description** : Il est important de suivre l'√©tat du trafic afin de pouvoir l'am√©liorer en proposant des d√©viations aux usagers en cas de forte affluence, ce qui permet de limiter le risque d'accidents et de sur-accidents qui ont entra√Æn√©s les jours de forte affluence.

2. **Corr√©lation entre l'√©tat du trafic Rennais et la m√©t√©o sur une m√™me p√©riode**
   **Objectif** : Analyser l‚Äôimpact des conditions m√©t√©orologiques et des niveaux de pollution sur le trafic routier afin de proposer des solutions pour am√©liorer la gestion de la mobilit√©, r√©duire les congestions et limiter les risques d‚Äôaccidents.
   - **Description** : Les conditions m√©t√©orologiques et la pollution influencent directement le trafic routier, impactant la s√©curit√©, la fluidit√© et les comportements des usagers. Analyser ces interactions permettrait d‚Äôoptimiser la gestion de la mobilit√© urbaine.


## D√©roulement Technique du Projet

### **√âtapes d'installation :**

1. **Cloner le d√©p√¥t :**
   ```bash
   git clone https://github.com/Coline-T/Transports_meteo
   cd Transports_meteo
   ```

2. **Cr√©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les d√©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

**Configurer les variables d'environnement :**
   Cr√©ez un fichier `.env` et renseignez les informations de connexion MongoDB , OPENAI , le topic kafka , le lien de l'api et Elasticsearch :
   ```env
MONGO_USERNAME="******"
MONGO_PASSWORD="******"
MONGO_DBNAME="*******"
MONGO_URI="*********"
API_URL=https://data.rennesmetropole.fr/api/explore/v2.1/catalog/datasets/etat-du-trafic-en-temps-reel/records
OPENAI_API_KEY="*******"
KAFKA_BROKER=localhost:9092"******"
KAFKA_TOPIC="*******"
   ```

### Sous-Projet : Ingestion et Pr√©paration des Donn√©es

Cette partie du projet est un sous-projet sp√©cifique √† l'ingestion et √† la pr√©paration des donn√©es, inclus dans notre projet global Transports_Meteo.

### Extraction et Ingestion
   - **Donn√©es des diff√©rents API** : Extraction des donn√©es sanitaires avec Python et envoi dans MongoDB.

### Traitement des donn√©es
   - **Traitement des donn√©es sous Python** : Les donn√©es stock√©es dans MongoDB sont trait√©s et transform√©s sous python.

### Stockage et Indexation avec Elasticsearch
   - Stockage des donn√©es des transports rennais, des donn√©es m√©t√©o et des donn√©es de la pollution de l'air dans Elasticsearch.

### Visualisation avec Kibana
   - Cr√©ation de tableaux de bord pour :
      - Mesure de l'affluence (voir si le trafic est plus souvent bouch√© ou libre)
      - Analyser la vitesse et le type du trafic en temps de pluie
      - ...

## Analyses et Indicateurs Attendus ---> A FAIRE

1. **...** : ...
2. **...** : ...
3. **...** : ...
4. **...** : ...

## Exemples de Cas d'Usage

- **Pour la m√©tropole rennaise** : Suivre l'√©tat du trafic rennais pour conna√Ætre les p√©riodes o√π les routes sont le plus emprunt√©s, tout en mettant ses donn√©es en lien avec la m√©t√©o du moment afin de pouvoir am√©liorer la mobilit√© urbaine mais aussi r√©duire le nombre d'accidents.
- **Pour les usagers des routes** : Les usagers pourront conna√Ætre les moments o√π il est d√©conseill√© d'emprunter certaines routes car bouch√© mais aussi conna√Ætre les impacts de la m√©t√©o sur le trafic. 
- **Pour les analystes** : Suivre les cons√©quences de circonstances ext√©rieurs tel que la m√©t√©o sur le trafic rennais.

## D√©ploiement

- **Docker** : Conteneurisation des services (Elasticsearch, Kibana) pour simplifier le d√©ploiement.
- **Configurations** : Variables d‚ÄôAPI et param√®tres de stockage configurables via des fichiers `.env`.
- **Automatisation** : Script de d√©ploiement pour ex√©cuter le pipeline complet.


## Visualisation des Donn√©es avec Kibana

Les donn√©es collect√©es et import√©es dans Elasticsearch  sont visualis√©es dans Kibana pour une analyse approfondie. Voici un aper√ßu de certaines visualisations cr√©√©es pour explorer les avis clients et leurs sentiments.

![alt text](tableau_de_bord.png)

![alt text](kibana1.png)

##  üìú Conclusion <a name="conclusion"></a>

L'application Realtime Restaurant Insights s'est av√©r√©e √™tre un atout consid√©rable pour les acteurs de la restauration cherchant √† comprendre et √† exploiter les retours clients en temps r√©el. Gr√¢ce √† l'int√©gration harmonieuse d'outils tels que Kafka pour l‚Äôingestion de donn√©es en temps r√©el, Apache Spark pour le traitement, et Elasticsearch et Kibana pour l‚Äôindexation et la visualisation, l'application permet une exploitation rapide et efficace des donn√©es critiques.

Cette solution offre aux restaurateurs une capacit√© in√©dite de suivre la satisfaction client, d‚Äôidentifier les probl√©matiques de mani√®re proactive, et de mettre en ≈ìuvre des actions correctives imm√©diates. Les gestionnaires de cha√Ænes peuvent obtenir une vue d‚Äôensemble de leurs multiples √©tablissements, facilitant une gestion centralis√©e tout en gardant un ≈ìil sur chaque restaurant. Cette vision consolid√©e am√©liore non seulement la qualit√© du service, mais permet aussi une prise de d√©cision fond√©e sur des informations v√©rifi√©es et actuelles.

En utilisant l‚ÄôAPI d‚ÄôOpenAI pour analyser les sentiments des avis clients, l'application est capable de transformer de simples commentaires en indicateurs concrets, fournissant des insights sur les aspects positifs et n√©gatifs du service et des produits. Cela aide non seulement √† rehausser l'exp√©rience client, mais permet √©galement aux √©quipes marketing d‚Äôorienter leurs strat√©gies de mani√®re plus personnalis√©e et pertinente.

Les fonctionnalit√©s de visualisation des donn√©es, avec Kibana, apportent une dimension interactive qui permet de transformer des volumes importants de donn√©es en tableaux de bord intuitifs. Ces visualisations permettent aux utilisateurs d'explorer les tendances, de suivre la satisfaction des clients en temps r√©el, et de prendre des d√©cisions √©clair√©es.

En somme, l‚Äôapplication "Realtime Restaurant Insights" se positionne comme un outil essentiel pour quiconque souhaite rester comp√©titif dans le secteur de la restauration. Elle aide √† optimiser la satisfaction client, am√©liorer la qualit√© des services, et exploiter les retours clients de mani√®re constructive. En mettant la donn√©e au centre de la prise de d√©cision, cette solution repr√©sente une avanc√©e majeure vers une gestion proactive et ax√©e sur les r√©sultats pour le secteur de la restauration.



üöß Difficult√©s Rencontr√©es

- **...** 
....

![alt text](image-1.png)

**Airflow**  est utilis√© pour orchestrer les pipelines de collecte de donn√©es via des DAGs. Un exemple de DAG est utilis√© pour envoyer nos donn√©es de MongoDB vers Kafka. Ce script Airflow s'ex√©cute toutes les 8 heures. Voici une images du  DAG :

![alt text](dag.png)

## Contributeurs

- Solenn COULON (@solennCoulon17): Data engineer -**solenn.coulon@supdevinci-edu.fr**
- Coline TREILLE (@Coline-T) : Data analyst -**coline.treille@supdevinci-edu.fr**


## Licence

Ce projet est sous licence MIT. N'h√©sitez pas √† utiliser et modifier le code pour vos propres projets.